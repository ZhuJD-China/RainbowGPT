import queue
import threading
import chromadb
import openai
import time
import os
from dotenv import load_dotenv
import gradio as gr
from langchain import hub
from langchain.agents import AgentExecutor, ZeroShotAgent
from langchain.agents.format_scratchpad import format_log_to_str, format_to_openai_function_messages
from langchain.agents.output_parsers import ReActJsonSingleInputOutputParser, OpenAIFunctionsAgentOutputParser
from langchain.chains.llm import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain.retrievers import ContextualCompressionRetriever, EnsembleRetriever
from langchain.retrievers.document_compressors import EmbeddingsFilter, DocumentCompressorPipeline
from langchain_community.document_transformers import EmbeddingsRedundantFilter
from langchain.tools import BaseTool
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.retrievers import BM25Retriever
from langchain_community.utilities import WolframAlphaAPIWrapper, ArxivAPIWrapper
from langchain_community.vectorstores import Chroma
from langchain_core.callbacks import FileCallbackHandler
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import Tool, render_text_description
from langchain_core.utils.function_calling import format_tool_to_openai_function
from langchain_text_splitters import CharacterTextSplitter
from loguru import logger
from langchain.chains import LLMMathChain
from langchain.callbacks.base import BaseCallbackHandler
from Rainbow_utils.model_config_manager import ModelConfigManager

# Rainbow_utils
from Rainbow_utils.get_tokens_cal_filter import filter_chinese_english_punctuation, num_tokens_from_string, \
    truncate_string_to_max_tokens, concatenate_if_dissimilar
from Rainbow_utils import get_google_result
from Rainbow_utils import get_prompt_templates
from Rainbow_utils.image_genearation import ImageGen


class RainbowKnowledge_Agent:
    def __init__(self):
        self.load_dotenv()
        self.initialize_variables()
        self.create_interface()
        # åˆå§‹åŒ–æ¨¡å‹é…ç½®ç®¡ç†å™¨
        self.model_manager = ModelConfigManager()
        # æ·»åŠ æœç´¢å†å²è®°å½•å™¨
        self.search_history = {
            "Google_Search": set(),
            "Local_Search": set(),
            "Calculator": set(),
            "Wolfram Alpha": set(),
            "Arxiv": set(),
            "Create_Image": set()
        }

    def load_dotenv(self):
        load_dotenv()

    def initialize_variables(self):
        self.docsearch_db = None
        self.script_name = os.path.basename(__file__)
        self.logfile = "./logs/" + self.script_name + ".log"
        logger.add(self.logfile,
                   format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
                   level="DEBUG",
                   rotation="1 MB",
                   compression="zip"
                   )
        self.handler = FileCallbackHandler(self.logfile)
        self.persist_directory = ".chromadb/"
        self.client = chromadb.PersistentClient(path=self.persist_directory)
        self.collection_name_select_global = None
        # local private llm name
        self.local_private_llm_name_global = None
        # private llm apis
        self.local_private_llm_api_global = None
        # private llm api key
        self.local_private_llm_key_global = None
        # http proxy
        self.proxy_url_global = None
        # åˆ›å»º ChatOpenAI å®ä¾‹ä½œä¸ºåº•å±‚è¯­è¨€æ¨¡å‹
        self.llm = None
        self.llm_name_global = None
        self.embeddings = None
        self.Embedding_Model_select_global = 0
        self.temperature_num_global = 0
        # æ–‡æ¡£åˆ‡åˆ†çš„é•¿åº¦
        self.input_chunk_size_global = None
        # æœ¬åœ°çŸ¥è¯†åº“åµŒå…¥token max
        self.local_data_embedding_token_max_global = None
        # åœ¨æ–‡ä»¶é¡¶éƒ¨å®šä¹‰docsearch_db
        self.docsearch_db = None
        self.human_input_global = None

        self.local_search_template = get_prompt_templates.local_search_template
        self.google_search_template = get_prompt_templates.google_search_template
        # å…¨å±€å·¥å…·åˆ—è¡¨åˆ›å»º
        self.tools = []
        # memory
        self.memory = ConversationBufferMemory(
            return_messages=True,
            memory_key="chat_history",
            output_key="output"
        )

        self.Google_Search_tool = None
        self.Local_Search_tool = None
        self.llm_Agent_checkbox_group = None
        self.intermediate_steps_log = ""

        # åˆå§‹åŒ–LLMç›¸å…³é…ç½®
        self.model_manager = ModelConfigManager()
        config = self.model_manager.get_active_config()
        
        # ä½¿ç”¨å…¨å±€é…ç½®åˆå§‹åŒ–Calculatorå·¥å…·çš„LLM
        base_llm = ChatOpenAI(
            model_name=config.model_name,
            openai_api_base=config.api_base,
            openai_api_key=config.api_key,
            temperature=0
        )
        llm_math = LLMMathChain.from_llm(llm=base_llm)
        
        def get_rainbow_agent_help(input_str: str) -> str:
            """æä¾› RainbowKnowledge_Agent çš„ä½¿ç”¨è¯´æ˜å’Œå¸®åŠ©"""
            help_text = """
            ğŸŒˆ RainbowKnowledge_Agent ä½¿ç”¨æŒ‡å—

            1. åŸºæœ¬åŠŸèƒ½
               - æ™ºèƒ½å¯¹è¯ï¼šæ”¯æŒè‡ªç„¶è¯­è¨€äº¤äº’
               - å¤šå·¥å…·é›†æˆï¼šGoogleæœç´¢ã€æœ¬åœ°çŸ¥è¯†åº“ã€Wolfram Alphaç­‰
               - çŸ¥è¯†åº“ç®¡ç†ï¼šæ”¯æŒå¤šä¸ªçŸ¥è¯†åº“åˆ‡æ¢å’ŒæŸ¥è¯¢
               - AIç»˜å›¾ï¼šæ”¯æŒå›¾ç‰‡ç”ŸæˆåŠŸèƒ½

            2. ä¸»è¦å·¥å…·è¯´æ˜
               - Google Searchï¼šå®æ—¶ç½‘ç»œæœç´¢
               - Local Knowledge Searchï¼šæœ¬åœ°çŸ¥è¯†åº“æŸ¥è¯¢
               - Wolfram Alphaï¼šæ•°å­¦å’Œç§‘å­¦è®¡ç®—
               - Arxivï¼šå­¦æœ¯è®ºæ–‡æœç´¢
               - Create Imageï¼šAIå›¾ç‰‡ç”Ÿæˆ

            3. ä½¿ç”¨æŠ€å·§
               - æ¸…æ™°è¡¨è¾¾é—®é¢˜éœ€æ±‚
               - åˆç†é€‰æ‹©å·¥å…·ç»„åˆ
               - çµæ´»è¿ç”¨è¿½é—®å’Œæ¾„æ¸…
               - æ³¨æ„ä¿æŒå¯¹è¯ä¸Šä¸‹æ–‡

            4. æœ€ä½³å®è·µ
               - å¤æ‚é—®é¢˜å»ºè®®åˆ†æ­¥æé—®
               - éœ€è¦æ—¶å¯è¯·æ±‚è¯¦ç»†è§£é‡Š
               - å–„ç”¨çŸ¥è¯†åº“åŠŸèƒ½
               - é€‚æ—¶åˆ‡æ¢Agentæ¨¡å¼

            è¾“å…¥"help [å·¥å…·å]"è·å–ç‰¹å®šå·¥å…·çš„è¯¦ç»†è¯´æ˜
            ä¾‹å¦‚ï¼šhelp Google Searchã€help Local Knowledge Search
            """
            
            # å¤„ç†ç‰¹å®šå·¥å…·çš„å¸®åŠ©è¯·æ±‚
            if input_str.lower().startswith('help '):
                tool_name = input_str.lower().replace('help ', '').strip()
                tool_helps = {
                    'rainbowagent_help': """
                    ğŸŒŸ RainbowAgent_Help å·¥å…·ä½¿ç”¨è¯´æ˜
                    
                    1. åŸºæœ¬ç”¨æ³•
                       - ç›´æ¥è¾“å…¥ "help" è·å–æ€»ä½“ä½¿ç”¨æŒ‡å—
                       - ä½¿ç”¨ "help [å·¥å…·å]" è·å–ç‰¹å®šå·¥å…·è¯´æ˜
                       - ä¾‹å¦‚: "help google search" æˆ– "help local knowledge search"
                    
                    2. æ”¯æŒçš„å·¥å…·è¯´æ˜æŸ¥è¯¢
                       - Google Search
                       - Local Knowledge Search
                       - Wolfram Alpha
                       - Arxiv
                       - Create Image
                    
                    3. å¸®åŠ©å†…å®¹åŒ…æ‹¬
                       - å·¥å…·åŠŸèƒ½ä»‹ç»
                       - é€‚ç”¨åœºæ™¯è¯´æ˜
                       - å…·ä½“ä½¿ç”¨æŠ€å·§
                       - æœ€ä½³å®è·µå»ºè®®
                    
                    4. ä½¿ç”¨å»ºè®®
                       - ä¼˜å…ˆæŸ¥çœ‹æ€»ä½“æŒ‡å—äº†è§£ç³»ç»ŸåŠŸèƒ½
                       - æ ¹æ®éœ€æ±‚æŸ¥è¯¢å…·ä½“å·¥å…·è¯´æ˜
                       - é‡åˆ°é—®é¢˜æ—¶æŸ¥çœ‹ç›¸å…³å·¥å…·çš„ä½¿ç”¨æŠ€å·§
                    """,
                    'google search': """
                    ğŸ” Google Search å·¥å…·ä½¿ç”¨è¯´æ˜
                    - åŠŸèƒ½ï¼šå®æ—¶è”ç½‘æœç´¢æœ€æ–°ä¿¡æ¯
                    - é€‚ç”¨åœºæ™¯ï¼šæŸ¥è¯¢å®æ—¶èµ„è®¯ã€è·å–ç½‘ç»œä¿¡æ¯
                    - ä½¿ç”¨æŠ€å·§ï¼š
                      1. ä½¿ç”¨ç²¾ç¡®å…³é”®è¯
                      2. å¯æŒ‡å®šæ—¶é—´èŒƒå›´
                      3. æ”¯æŒå¤šè¯­è¨€æœç´¢
                    """,
                    'local knowledge search': """
                    ğŸ“š Local Knowledge Search å·¥å…·ä½¿ç”¨è¯´æ˜
                    - åŠŸèƒ½ï¼šæœç´¢æœ¬åœ°çŸ¥è¯†åº“å†…å®¹
                    - é€‚ç”¨åœºæ™¯ï¼šä¸“ä¸šé¢†åŸŸæŸ¥è¯¢ã€æ–‡æ¡£å†…å®¹æ£€ç´¢
                    - ä½¿ç”¨æŠ€å·§ï¼š
                      1. é€‰æ‹©åˆé€‚çš„çŸ¥è¯†åº“
                      2. ä½¿ç”¨å‡†ç¡®çš„å…³é”®è¯
                      3. å¯è¿›è¡Œä¸Šä¸‹æ–‡å…³è”æœç´¢
                    """,
                    'wolfram alpha': """
                    ğŸ§® Wolfram Alpha å·¥å…·ä½¿ç”¨è¯´æ˜
                    - åŠŸèƒ½ï¼šæ•°å­¦è®¡ç®—ä¸ç§‘å­¦åˆ†æ
                    - é€‚ç”¨åœºæ™¯ï¼š
                      1. å¤æ‚æ•°å­¦è®¡ç®—
                      2. ç§‘å­¦æ•°æ®æŸ¥è¯¢
                      3. å·¥ç¨‹è®¡ç®—é—®é¢˜
                    - ä½¿ç”¨æŠ€å·§ï¼š
                      1. ä½¿ç”¨æ¸…æ™°çš„è‹±æ–‡è¡¨è¾¾
                      2. æä¾›å®Œæ•´çš„è®¡ç®—æ¡ä»¶
                      3. é€‚åˆå¤„ç†å®šé‡åˆ†æ
                    """,
                    'arxiv': """
                    ğŸ“– Arxiv å·¥å…·ä½¿ç”¨è¯´æ˜
                    - åŠŸèƒ½ï¼šå­¦æœ¯è®ºæ–‡æœç´¢å’Œè·å–
                    - é€‚ç”¨åœºæ™¯ï¼š
                      1. ç ”ç©¶è®ºæ–‡æŸ¥è¯¢
                      2. å­¦æœ¯åŠ¨æ€è·Ÿè¸ª
                      3. ä¸“ä¸šçŸ¥è¯†è·å–
                    - ä½¿ç”¨æŠ€å·§ï¼š
                      1. ä½¿ç”¨å‡†ç¡®çš„å­¦æœ¯å…³é”®è¯
                      2. å¯æŒ‰é¢†åŸŸç­›é€‰
                      3. å…³æ³¨æœ€æ–°å‘è¡¨æ—¶é—´
                    """,
                    'create image': """
                    ğŸ¨ Create Image å·¥å…·ä½¿ç”¨è¯´æ˜
                    - åŠŸèƒ½ï¼šAIå›¾ç‰‡ç”Ÿæˆ
                    - é€‚ç”¨åœºæ™¯ï¼š
                      1. åˆ›æ„å›¾ç‰‡ç”Ÿæˆ
                      2. è§†è§‰å†…å®¹åˆ›ä½œ
                      3. å›¾ç‰‡é£æ ¼è½¬æ¢
                    - ä½¿ç”¨æŠ€å·§ï¼š
                      1. æä¾›è¯¦ç»†çš„å›¾ç‰‡æè¿°
                      2. æŒ‡å®šå…·ä½“çš„é£æ ¼è¦æ±‚
                      3. å¯ä»¥å‚è€ƒæ ·ä¾‹å›¾ç‰‡
                    """
                }
                return tool_helps.get(tool_name, "æœªæ‰¾åˆ°è¯¥å·¥å…·çš„å…·ä½“è¯´æ˜ï¼Œè¯·æ£€æŸ¥å·¥å…·åç§°æ˜¯å¦æ­£ç¡®ã€‚\nå¯ç”¨çš„å·¥å…·åç§°ï¼šrainbowagent_help, google search, local knowledge search, wolfram alpha, arxiv, create image")
            
            return help_text

        self.math_tool = Tool(
            name="RainbowAgent_Help",
            func=get_rainbow_agent_help,
            description="""RainbowKnowledge_Agent å¸®åŠ©å·¥å…·ï¼Œå¯ç”¨äºï¼š
1. è·å–ç³»ç»Ÿæ•´ä½“ä½¿ç”¨è¯´æ˜
2. æŸ¥è¯¢ç‰¹å®šå·¥å…·çš„è¯¦ç»†è¯´æ˜
3. äº†è§£ä½¿ç”¨æŠ€å·§å’Œæœ€ä½³å®è·µ
ä½¿ç”¨æ–¹å¼ï¼š
- ç›´æ¥è¯¢é—®ä½¿ç”¨è¯´æ˜
- è¾“å…¥"help [å·¥å…·å]"è·å–ç‰¹å®šå·¥å…·è¯´æ˜"""
        )

        # åœ¨åˆå§‹åŒ– math_tool ä¹‹åæ·»åŠ  wolfram_tool çš„åˆå§‹åŒ–
        try:
            wolfram = WolframAlphaAPIWrapper(
                wolfram_alpha_appid=os.getenv('WOLFRAM_ALPHA_APPID')
            )
            self.wolfram_tool = Tool(
                name="Wolfram Alpha",
                func=wolfram.run,
                description="""ç”¨äºè§£å†³æ•°å­¦ã€ç§‘å­¦å’Œå·¥ç¨‹è®¡ç®—çš„å¼ºå¤§å·¥å…·ã€‚é€‚ç”¨äº:
1. å¤æ‚æ•°å­¦è®¡ç®—å’Œæ–¹ç¨‹æ±‚è§£
2. ç§‘å­¦æ•°æ®æŸ¥è¯¢å’Œåˆ†æ
3. å•ä½æ¢ç®—å’Œç‰©ç†è®¡ç®—
4. ç»Ÿè®¡å’Œæ•°æ®åˆ†æ
ä½¿ç”¨æ—¶è¯·ç”¨æ¸…æ™°çš„è‹±æ–‡æè¿°é—®é¢˜ã€‚
ç¤ºä¾‹: 'solve x^2 + 2x + 1 = 0' æˆ– 'distance from Earth to Mars'"""
            )
        except Exception as e:
            logger.error(f"Failed to initialize Wolfram Alpha tool: {str(e)}")
            self.wolfram_tool = None

    def get_llm(self):
        """è·å–å½“å‰é…ç½®çš„LLMå®ä¾‹"""
        config = self.model_manager.get_active_config()
        return ChatOpenAI(
            model_name=config.model_name,
            openai_api_base=config.api_base,
            openai_api_key=config.api_key,
            temperature=config.temperature,
            streaming=True
        )

    def check_search_history(self, tool_name, query):
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨é‡å¤æœç´¢"""
        if query in self.search_history[tool_name]:
            return True
        self.search_history[tool_name].add(query)
        return False

    def ask_local_vector_db(self, question):
        # æ£€æŸ¥æ˜¯å¦é‡å¤æœç´¢
        if self.check_search_history("Local_Search", question):
            return "âš ï¸ æ£€æµ‹åˆ°é‡å¤æœç´¢ã€‚è¯·å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯æˆ–å…¶ä»–å·¥å…·æ¥è·å–æ–°ä¿¡æ¯ã€‚"
        
        # ä½¿ç”¨æ¨¡å‹é…ç½®ç®¡ç†å™¨è·å–LLM
        self.llm = self.get_llm()
        
        local_search_prompt = PromptTemplate(
            input_variables=["combined_text", "human_input", "human_input_first"],
            template=self.local_search_template,
        )
        
        local_chain = LLMChain(
            llm=self.llm,
            prompt=local_search_prompt,
            verbose=True,
        )

        docs = []
        if self.Embedding_Model_select_global == 0:
            print("OpenAIEmbeddings Search")
            # ç»“åˆåŸºç¡€æ£€ç´¢å™¨+Embeddingä¸Šä¸‹æ–‡å‹ç¼©
            # å°†ç¨€ç–æ£€ç´¢å™¨ï¼ˆå¦‚ BM25ï¼‰ä¸å¯†é›†æ£€ç´¢å™¨ï¼ˆå¦‚åµŒå…¥ç›¸ä¼¼æ€§ï¼‰ç›¸ç»“åˆ
            chroma_retriever = self.docsearch_db.as_retriever(search_kwargs={"k": 30})

            # å°†ç¼©å™¨å’Œæ–‡æ¡£è½¬æ¢å™¨ä¸²åœ¨ä¸€èµ·
            splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator=". ")
            redundant_filter = EmbeddingsRedundantFilter(embeddings=self.embeddings)
            relevant_filter = EmbeddingsFilter(embeddings=self.embeddings, similarity_threshold=0.76)
            pipeline_compressor = DocumentCompressorPipeline(
                transformers=[splitter, redundant_filter, relevant_filter]
            )
            compression_retriever = ContextualCompressionRetriever(base_compressor=pipeline_compressor,
                                                                   base_retriever=chroma_retriever)
            # compressed_docs = compression_retriever.get_relevant_documents(question, tools=tools)

            the_collection = self.client.get_collection(name=self.collection_name_select_global)
            the_metadata = the_collection.get()
            the_doc_llist = the_metadata['documents']
            bm25_retriever = BM25Retriever.from_texts(the_doc_llist)
            bm25_retriever.k = 30

            # ç½®æ¬¡æ•°
            max_retries = 3
            retries = 0
            while retries < max_retries:
                try:
                    # åˆå§‹åŒ– ensemble æ£€ç´¢å™¨
                    ensemble_retriever = EnsembleRetriever(
                        retrievers=[bm25_retriever, compression_retriever], weights=[0.5, 0.5]
                    )
                    docs = ensemble_retriever.get_relevant_documents(question)
                    break  # å¦‚æœæˆåŠŸæ‰§è¡Œï¼Œè·³å‡ºå¾ªç¯
                except openai.error.OpenAIError as openai_error:
                    if "Rate limit reached" in str(openai_error):
                        print(f"Rate limit reached: {openai_error}")
                        # å¦‚æœæ˜¯é€Ÿç‡é™åˆ¶é”™è¯¯ï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
                        time.sleep(20)
                        retries += 1
                    else:
                        print(f"OpenAI API error: {openai_error}")
                        docs = []
                        break  # å¦‚æœé‡åˆ°å…¶ä»–é”™è¯¯ï¼Œè·³å‡ºå¾ªç¯
            # å¤„ç†å¾ªç¯ç»“æŸåçš„æƒ…å†µ
            if retries == max_retries:
                print(f"Max retries reached. Code execution failed.")
        elif self.Embedding_Model_select_global == 1:
            print("HuggingFaceEmbedding Search")
            chroma_retriever = self.docsearch_db.as_retriever(search_kwargs={"k": 30})
            # docs = chroma_retriever.get_relevant_documents(question)
            # chroma_vectorstore = Chroma.from_texts(the_doc_llist, embeddings)
            # chroma_retriever = chroma_vectorstore.as_retriever(search_kwargs={"k": 10})

            the_collection = self.client.get_collection(name=self.collection_name_select_global)
            the_metadata = the_collection.get()
            the_doc_llist = the_metadata['documents']
            bm25_retriever = BM25Retriever.from_texts(the_doc_llist)
            bm25_retriever.k = 30

            # åˆå§‹åŒ– ensemble æ£€ç´¢å™¨
            ensemble_retriever = EnsembleRetriever(
                retrievers=[bm25_retriever, chroma_retriever], weights=[0.5, 0.5]
            )
            docs = ensemble_retriever.get_relevant_documents(question)

        cleaned_matches = []
        total_toknes = 0
        last_index = 0
        for index, context in enumerate(docs):
            cleaned_context = context.page_content.replace('\n', ' ').strip()
            cleaned_context = f"{cleaned_context}"
            tokens = num_tokens_from_string(cleaned_context, "cl100k_base")
            # tokens = tokenizers.encode(cleaned_context, add_special_tokens=False)
            if total_toknes + tokens <= (int(self.local_data_embedding_token_max_global)):
                cleaned_matches.append(cleaned_context)
                total_toknes += tokens
            else:
                last_index = index
                break
        print("Embeddingäº† ", str(last_index + 1), " ä¸ªçŸ¥è¯†åº“æ–‡æ¡£å—")
        # å°†æ¸…ç†è¿‡çš„åŒ¹é…é¡¹ç»„åˆåˆæˆä¸€ä¸ªå­—ç¬¦ä¸²
        combined_text = " ".join(cleaned_matches)

        answer = local_chain.predict(combined_text=combined_text, human_input=question,
                                     human_input_first=self.human_input_global)
        return answer

    def createImageByBing(self, input):
        auth_cooker = os.getenv('BINGCOKKIE')
        sync_gen = ImageGen(auth_cookie=auth_cooker)
        image_list = sync_gen.get_images(input)
        response = []
        if image_list is None:
            return "æˆ‘æ— æ³•ä¸ºæ‚¨ç”Ÿæˆå¯¹åº”çš„å›¾ç‰‡ï¼Œè¯·é‡è¯•æˆ–è€…è¡¥å……æ‚¨çš„æè¿°"
        else:
            for url in image_list:
                if not url.endswith(".svg"):
                    response.append(url)
            return response

    def get_google_answer(self, question, result_queue):
        try:
            logger.debug("Starting get_google_answer")
            google_answer_box = get_google_result.selenium_google_answer_box(
                question, "Rainbow_utils/chromedriver.exe")
            google_answer_box = filter_chinese_english_punctuation(google_answer_box)
            result_queue.put(("google_answer_box", google_answer_box))
            logger.debug("Completed get_google_answer successfully")
        except Exception as e:
            logger.exception(f"Error in get_google_answer: {str(e)}")
            result_queue.put(("google_answer_box", f"è·å–Googleç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}"))

    def process_data_title_summary(self, data_title_Summary, result_queue):
        data_title_Summary_str = ''.join(data_title_Summary)
        result_queue.put(("data_title_Summary_str", data_title_Summary_str))

    def process_custom_search_link(self, custom_search_link, result_queue):
        link_detail_res = []
        for link in custom_search_link[:1]:
            website_content = get_google_result.get_website_content(link)
            if website_content:
                link_detail_res.append(website_content)

        link_detail_string = '\n'.join(link_detail_res)
        link_detail_string = filter_chinese_english_punctuation(link_detail_string)
        result_queue.put(("link_detail_string", link_detail_string))

    def custom_search_and_fetch_content(self, question, result_queue):
        try:
            logger.debug("Starting custom_search_and_fetch_content")
            custom_search_link, data_title_Summary = get_google_result.google_custom_search(question)

            thread3 = threading.Thread(target=self.process_data_title_summary,
                                       args=(data_title_Summary, result_queue))
            thread4 = threading.Thread(target=self.process_custom_search_link,
                                       args=(custom_search_link, result_queue))

            thread3.start()
            thread4.start()

            # Add timeout to thread joins
            thread3.join(timeout=30)
            thread4.join(timeout=30)

            if thread3.is_alive() or thread4.is_alive():
                logger.error("Content processing threads timed out")
                raise TimeoutError("Content processing timed out")

            logger.debug("Completed custom_search_and_fetch_content successfully")
        except Exception as e:
            logger.exception(f"Error in custom_search_and_fetch_content: {str(e)}")
            result_queue.put(("data_title_Summary_str", f"è·å–æœç´¢å†…å®¹æ—¶å‡ºé”™: {str(e)}"))
            result_queue.put(("link_detail_string", ""))

    def Google_Search_run(self, question):
        try:
            # æ£€æŸ¥æ˜¯å¦é‡å¤æœç´¢
            if self.check_search_history("Google_Search", question):
                return "âš ï¸ æ£€æµ‹åˆ°é‡å¤æœç´¢ã€‚è¯·å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯æˆ–å…¶ä»–å·¥å…·æ¥è·å–æ–°ä¿¡æ¯ã€‚"
            
            logger.debug(f"Starting Google search for question: {question}")
            
            # ä½¿ç”¨æ¨¡å‹é…ç½®ç®¡ç†å™¨è·å–LLM
            self.llm = self.get_llm()
            
            local_search_prompt = PromptTemplate(
                input_variables=["combined_text", "human_input", "human_input_first"],
                template=self.google_search_template,
            )
            
            local_chain = LLMChain(
                llm=self.llm,
                prompt=local_search_prompt,
                verbose=True,
            )

            # åˆ›å»ºä¸€ä¸ªé˜Ÿåˆ—æ¥å­˜å‚¨çº¿ç¨‹ç»“æœ
            results_queue = queue.Queue()
            # åˆ›å»ºå¹¶å¯çº¿ç¨‹
            thread1 = threading.Thread(target=self.get_google_answer, args=(question, results_queue))
            thread2 = threading.Thread(target=self.custom_search_and_fetch_content, args=(question, results_queue))

            logger.debug("Starting search threads")
            thread1.start()
            thread2.start()

            # Add timeout to thread joins
            thread1.join(timeout=30)
            thread2.join(timeout=30)

            if thread1.is_alive() or thread2.is_alive():
                logger.error("Search threads timed out")
                raise TimeoutError("Search operation timed out")

            # åˆå§‹åŒ–å˜é‡
            google_answer_box = ""
            data_title_Summary_str = ""
            link_detail_string = ""

            # æå–å¹¶åˆ†é…ç»“æœ
            while not results_queue.empty():
                result_type, result = results_queue.get()
                if result_type == "google_answer_box":
                    google_answer_box = result
                elif result_type == "data_title_Summary_str":
                    data_title_Summary_str = result
                elif result_type == "link_detail_string":
                    link_detail_string = result

            finally_combined_text = f"""
            å½“å‰å…³é”®å­—æœç´¢çš„ç­”æ¡ˆæ¡†æ®ï¼š
            {google_answer_box}

            æœç´¢ç»“æœç›¸ä¼¼åº¦TOP10çš„ç½‘ç«™æ ‡é¢˜å’Œæ‘˜è¦æ•°æ®ï¼š
            {data_title_Summary_str}

            æœç´¢ç»“æœç›¸ä¼¼åº¦TOP1çš„ç½‘ç«™çš„è¯¦ç»†å†…å®¹æ•°æ®:
            {link_detail_string}

            """

            truncated_text = truncate_string_to_max_tokens(finally_combined_text,
                                                           self.local_data_embedding_token_max_global,
                                                           "cl100k_base",
                                                           step_size=256)

            answer = local_chain.predict(combined_text=truncated_text, human_input=question,
                                         human_input_first=self.human_input_global)

            return answer

        except Exception as e:
            logger.exception(f"Error in Google_Search_run: {str(e)}")
            return f"æœç´¢è¿‡ç¨‹ä¸­ç”Ÿé”™è¯¯: {str(e)}ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–é‡è¯•ã€‚"

    def echo(self, message, history, collection_name_select, print_speed_step,
             tool_checkbox_group, Embedding_Model_select, local_data_embedding_token_max,
             llm_Agent_checkbox_group):
        """
        ä¿ç•™llm_Agent_checkbox_groupå‚æ•°
        """
        # é‡ç½®æœç´¢å†å²
        self.search_history = {
            "Google_Search": set(),
            "Local_Search": set(),
            "Calculator": set(),
            "Wolfram Alpha": set(),
            "Arxiv": set(),
            "Create_Image": set()
        }
        
        self.human_input_global = message
        self.collection_name_select_global = str(collection_name_select)
        self.local_data_embedding_token_max_global = int(local_data_embedding_token_max)
        self.llm_Agent_checkbox_group = llm_Agent_checkbox_group  # ä¿ç•™Agentç±»å‹è®¾ç½®

        # è·å–å½“å‰é…ç½®
        config = self.model_manager.get_active_config()
        
        response = (f"{config.model_name} æ¨¡å‹åŠ è½½ä¸­..... temperature={config.temperature}")
        for i in range(0, len(response), int(print_speed_step)):
            yield response[: i + int(print_speed_step)]

        # åˆå§‹åŒ–LLM
        self.llm = self.get_llm()

        self.tools = []  # é‡ç½®å·¥å…·åˆ—è¡¨
        # Check if 'wolfram-alpha' is in the selected tools
        if "wolfram-alpha" in tool_checkbox_group:
            wolfram = WolframAlphaAPIWrapper()
            wolfram_tool = Tool(
                name="Wolfram Alpha",
                func=wolfram.run,
                description="Useful for when you need to answer questions about Math, Science, Technology, Culture, Society and Everyday Life"
            )
            self.tools.append(wolfram_tool)

        if "arxiv" in tool_checkbox_group:
            arxiv = ArxivAPIWrapper()
            arxiv_tool = Tool(
                name="Arxiv",
                func=arxiv.run,
                description="Useful for when you need to get information about scientific papers from arxiv.org. Input should be a search query."
            )
            self.tools.append(arxiv_tool)

        self.Google_Search_tool = Tool(
            name="Google_Search",
            func=self.Google_Search_run,
            description="""
                è¿™æ˜¯ä¸€ä¸ªå¦‚æœæœ¬åœ°çŸ¥è¯†åº“æ— ç­”æ¡ˆæˆ–é—®é¢˜éœ€è¦ç½‘ç»œæœç´¢çš„Googleæœç´¢å·¥å…·ã€‚
                1.ä½ å…ˆæ ¹æ®æˆ‘çš„é—®é¢˜æå–å‡ºæœ€é€‚åˆGoogleæœç´¢å¼•æ“æœç´¢çš„å…³é”®å­—è¿›è¡Œæœç´¢,å¯ä»¥é€‰æ‹©è‹±è¯­æˆ–è€…ä¸­æ–‡æœç´¢
                2.åŒæ—¶å¢åŠ ä¸€äº›æœç´¢æç¤ºè¯åŒ…æ‹¬(å¼•å·ï¼Œæ—¶é—´ï¼Œå…³é”®å­—)
                3.å¦‚æœé—®é¢˜æ¯”è¾ƒå¤æ‚ï¼Œä½ å¯ä»¥ä¸€æ­¥ä¸€æ­¥çš„æ€è€ƒå»æœç´¢å’Œå›ç­”
                4.ç¡®ä¿æ¯ä¸ªå›ç­”éƒ½ä¸ä»…åŸºäºæ•°æ®ï¼Œè¾“å‡ºçš„å›ç­”å¿…é¡»åŒ…å«æ·±å…¥ã€å®Œæ•´ï¼Œå……åˆ†åæ˜ ä½ å¯¹é—®é¢˜çš„å…¨é¢ç†è§£ã€‚
            """
        )
        self.Local_Search_tool = Tool(
            name="Local_Search",
            func=self.ask_local_vector_db,
            description="""
                è¿™æ˜¯ä¸€ä¸ªæœ¬åœ°çŸ¥è¯†åº“æœç´¢å·¥å…·ï¼Œä½ å¯ä»¥ä¼˜ä½¿ç”¨æœ¬åœ°æœç´¢å¹¶æ€»ç»“å›ç­”ã€‚
                1.ä½ å…ˆæ ¹æˆ‘çš„é—®é¢˜æå–å‡ºæœ€é€‚åˆembeddingæ¨¡å‹å‘é‡åŒ¹é…çš„å…³é”®å­—è¿›è¡Œæœç´¢ã€‚
                2.æ³¨æ„ä½ éœ€è¦æå‡ºéå¸¸æœ‰é’ˆå¯¹æ€§å‡†ç¡®çš„é—®é¢˜å’Œå›ç­”ã€‚
                3.å¦‚æœé—®é¢˜æ¯”è¾ƒå¤æ‚ï¼Œå¯ä»¥å°†å¤æ‚çš„é—®é¢˜è¿›è¡Œè¡Œæ‹†åˆ†ï¼Œä½ å¯ä»¥ä¸€æ­¥ä¸€æ­¥çš„æ€è€ƒã€‚
                4.ç¡®ä¿æ¯ä¸ªå›ç­”éƒ½ä¸ä»…åŸºäºæ•°æ®ï¼Œè¾“å‡ºçš„å›ç­”å¿…é¡»åŒ…å«æ·±å…¥ã€å®Œæ•´ï¼Œå……åˆ†åæ˜ ä½ å¯¹é—®é¢˜çš„å…¨é¢ç†è§£ã€‚
            """
        )

        self.Create_Image_tool = Tool(
            name="Create_Image",
            func=self.createImageByBing,
            description="""
                è¿™æ˜¯ä¸€ä¸ªå›¾ç‰‡ç”Ÿæˆå·¥å…·ï¼Œå½“æˆ‘çš„é—®é¢˜ä¸­æ˜ç¡®éœ€è¦ç”»å›¾ï¼Œä½ å°±å¯ä»¥ä½¿ç”¨è¯¥å·¥å…·å¹¶ç”Ÿæˆå›¾ç‰‡
                1ã€‚å½“ä½ å›å…³äºéœ€è¦ä½¿ç”¨bingæ¥ç”Ÿæˆä»€ä¹ˆç”»å›¾ã€ç…§ç‰‡æ—¶å¾ˆæœ‰ç”¨ï¼Œå…ˆæå–ç”Ÿæˆå›¾ç‰‡çš„æç¤ºè¯ï¼Œç„¶åè°ƒç”¨è¯¥å·¥å…·ã€‚
                2.å¹¶ä¸¥æ ¼æŒ‰ç…§Markdownè¯­æ³•: [![å›¾ç‰‡æè¿°](å›¾ç‰‡é“¾æ¥)](å›¾ç‰‡é“¾æ¥)ã€‚
                3.å¦‚æœç”Ÿæˆçš„å›¾ç‰‡é“¾æ¥æ•°é‡å¤§äº1ï¼Œå°†å…¶å…¨éƒ¨ä¸¥æ ¼æŒ‰ç…§Markdownè¯­æ³•: [![å›¾ç‰‡æè¿°](å›¾ç‰‡é“¾æ¥)](å›¾ç‰‡é“¾æ¥)ã€‚
                4.å¦‚æœé—®é¢˜æ¯”è¾ƒå¤æ‚ï¼Œå¯ä»¥å°†å¤æ‚çš„é—®é¢˜è¿›è¡Œæ‹†åˆ†ï¼Œä½ å¯ä»¥ä¸€æ­¥ä¸€æ­¥çš„æ€è€ƒã€‚
                """
        )

        self.tools = [self.math_tool]  # ç¡®ä¿å§‹ç»ˆåŒ…å« Calculator å·¥å…·
        # Initialize flags for additional tools
        flag_get_Local_Search_tool = False
        # Check for additional tools and append them if not already in the list
        for tg in tool_checkbox_group:
            if tg == "wolfram-alpha" and self.wolfram_tool is not None:
                response = "Wolfram Alpha å·¥å…·åŠ å…¥ å›ç­”ä¸­..........."
                for i in range(0, len(response), int(print_speed_step)):
                    yield response[:i + int(print_speed_step)]
                self.tools.append(self.wolfram_tool)
                
            elif tg == "Google Search" and self.Google_Search_tool not in self.tools:
                response = "Google Search å·¥å…·åŠ å…¥ å›ç­”ä¸­..........."
                for i in range(0, len(response), int(print_speed_step)):
                    yield response[:i + int(print_speed_step)]
                self.tools.append(self.Google_Search_tool)

            elif tg == "Local Knowledge Search" and self.Local_Search_tool not in self.tools:
                response = "Local Knowledge Search å·¥å…·åŠ å…¥ å›ç­”ä¸­..........."
                for i in range(0, len(response), int(print_speed_step)):
                    yield response[:i + int(print_speed_step)]
                self.tools.append(self.Local_Search_tool)

                response = (f"{config.model_name} & {Embedding_Model_select} æ¨¡å‹åŠ è½½ä¸­.....temperature="
                            + str(self.temperature_num_global))
                for i in range(0, len(response), int(print_speed_step)):
                    yield response[:i + int(print_speed_step)]

                if Embedding_Model_select in ["Openai Embedding", "", None]:
                    self.embeddings = OpenAIEmbeddings()
                    self.embeddings.show_progress_bar = True
                    self.embeddings.request_timeout = 20
                    self.Embedding_Model_select_global = 0
                elif Embedding_Model_select == "HuggingFace Embedding":
                    self.embeddings = HuggingFaceEmbeddings(
                        model_name="sentence-transformers/all-mpnet-base-v2",
                        cache_folder="models"
                    )
                    self.Embedding_Model_select_global = 1

                flag_get_Local_Search_tool = True

            elif tg == "Create Image" and self.Create_Image_tool not in self.tools:
                response = "Create Image å·¥å…·åŠ å…¥ å›ç­”ä¸­..........."
                for i in range(0, len(response), int(print_speed_step)):
                    yield response[:i + int(print_speed_step)]
                self.tools.append(self.Create_Image_tool)

        if message == "":
            response = "å“å‘€ï¼å¥½åƒæœ‰ç‚¹å°å°´å°¬ï¼Œæ‚¨ä¼¼ä¹å¿˜è®°æå‡ºé—®é¢˜äº†ã€‚åˆ«ç€æ€¥ï¼Œéšæ—¶è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ‘å°†å°½åŠ›ä¸ºæ‚¨æä¾›å¸®åŠ©ï¼"
            for i in range(0, len(response), int(print_speed_step)):
                yield response[: i + int(print_speed_step)]
            return

        if flag_get_Local_Search_tool:
            if collection_name_select and collection_name_select != "...":
                print(f"{collection_name_select}", " Collection exists, load it")
                response = f"{collection_name_select}" + "çŸ¥è¯†åº“åŠ è½½ä¸­ï¼Œè¯·ç­‰å¾…æˆ‘çš„å›ç­”......."
                for i in range(0, len(response), int(print_speed_step)):
                    yield response[: i + int(print_speed_step)]
                self.docsearch_db = Chroma(client=self.client, embedding_function=self.embeddings,
                                           collection_name=collection_name_select)
            else:
                response = "æœªé€‰æ‹©çŸ¥è¯†åº“ï¼Œå›ç­”ä¸­æ­¢ã€‚"
                for i in range(0, len(response), int(print_speed_step)):
                    yield response[: i + int(print_speed_step)]
                return

        if llm_Agent_checkbox_group == "openai-functions":
            prompt = ChatPromptTemplate.from_messages([
                ("system", "You are a helpful assistant"),
                ("user", "{input}"),
                MessagesPlaceholder(variable_name="agent_scratchpad"),
            ])

            llm_with_tools = self.llm.bind(
                functions=[format_tool_to_openai_function(t) for t in self.tools]
            )

            agent = (
                    {
                        "input": lambda x: x["input"],
                        "agent_scratchpad": lambda x: format_to_openai_function_messages(
                            x["intermediate_steps"]
                        ),
                    }
                    | prompt
                    | llm_with_tools
                    | OpenAIFunctionsAgentOutputParser()
            )

            agent_executor = AgentExecutor(
                agent=agent,
                tools=self.tools,
                verbose=True,
                return_intermediate_steps=True,
                handle_parsing_errors=True,
            )

            try:
                response = ""
                for chunk in agent_executor.stream({"input": message}):
                    if "output" in chunk:
                        response += chunk["output"]
                        yield response
                    elif "intermediate_step" in chunk:
                        self.intermediate_steps_log = str(chunk["intermediate_step"])

            except Exception as e:
                yield f"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
                logger.error(f"Error in agent execution: {str(e)}")
        elif llm_Agent_checkbox_group == "ZeroShotAgent-memory":
            # ä¿®æ”¹ prefix å’Œ suffix ä»¥æ›´å¥½åœ°å¤„ç†å¯¹è¯
            prefix = """ä½ æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„AIåŠ©æ‰‹ã€‚è¯·éµå¾ªä»¥ä¸‹åŸåˆ™:

1. é¿å…é‡å¤æŸ¥è¯¢
   - ä¸è¦é‡å¤ä½¿ç”¨ç›¸åŒçš„æœç´¢å…³é”®è¯
   - ä¸è¦é‡å¤æŸ¥è¯¢å·²è·å¾—çš„ä¿¡æ¯
   - å¦‚éœ€è¡¥å……ä¿¡æ¯ï¼Œä½¿ç”¨ä¸åŒè§’åº¦çš„å…³é”®è¯

2. å·¥å…·ä½¿ç”¨è§„åˆ™
   - æ¯ä¸ªå·¥å…·æœ€å¤šä½¿ç”¨1æ¬¡
   - ä»…åœ¨å¿…è¦æ—¶ä½¿ç”¨å·¥å…·
   - å·²æœ‰è¶³å¤Ÿä¿¡æ¯æ—¶ç›´æ¥å›ç­”
   - æœ€å¤šä½¿ç”¨2æ¬¡å·¥å…·æŸ¥è¯¢

3. å›ç­”è¦æ±‚
   - ä¿¡æ¯å®Œæ•´å‡†ç¡®
   - é€»è¾‘æ¸…æ™°è¿è´¯
   - ç›´æ¥ç»™å‡ºç­”æ¡ˆ
   - é¿å…å†—ä½™å†…å®¹

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›å¤:

Thought: ç®€è¦åˆ†æå½“å‰æƒ…å†µ

Action: å·¥å…·åç§°

Action Input: æŸ¥è¯¢å†…å®¹

Observation: å·¥å…·è¿”å›ç»“æœ

Final Answer: å®Œæ•´ç­”æ¡ˆ

å½“å‰å¯ç”¨å·¥å…·:"""

            suffix = """æ³¨æ„äº‹é¡¹:
1. ç¦æ­¢é‡å¤æŸ¥è¯¢
2. æœ€å¤šä½¿ç”¨2æ¬¡å·¥å…·
3. è·å¾—ç­”æ¡ˆç«‹å³åœæ­¢

å†å²å¯¹è¯:
{chat_history}

å½“å‰é—®é¢˜: {input}

{agent_scratchpad}"""

            prompt = ZeroShotAgent.create_prompt(
                self.tools,
                prefix=prefix,
                suffix=suffix,
                input_variables=["input", "chat_history", "agent_scratchpad"],
            )

            llm_chain = LLMChain(llm=self.llm, prompt=prompt)
            agent = ZeroShotAgent(llm_chain=llm_chain, tools=self.tools, verbose=True)

            # åˆ›å»ºä¸€ä¸ªå›è°ƒå¤„ç†å™¨æ¥æ•è·ä¸­é—´æ­¥éª¤
            class VerboseHandler(BaseCallbackHandler):
                def __init__(self):
                    self.steps = []
                    self.current_iteration = 0
                    self.current_output = ""
                    super().__init__()
                
                def on_agent_action(self, action, color=None, **kwargs):
                    try:
                        # å¢åŠ è½®æ¬¡è®¡æ•°å’Œæ€è€ƒè¿‡ç¨‹è®°å½•
                        self.current_iteration += 1
                        # ä½¿ç”¨ Markdown æ ¼å¼æ¥çªå‡ºæ˜¾ç¤ºè½®æ¬¡ä¿¡æ¯
                        step_text = f"\n### ğŸ¤” ç¬¬ {self.current_iteration} è½®æ€è€ƒè¿‡ç¨‹\n\n"
                        
                        # è®°å½•æ€è€ƒè¿‡ç¨‹ï¼Œä½¿ç”¨æ›´é†’ç›®çš„æ ¼å¼
                        if hasattr(action, 'log') and action.log:
                            step_text += f"ğŸ” **æ€è€ƒ:** {action.log}\n\n"
                        
                        # è®°å½•å·¥å…·ä½¿ç”¨
                        if hasattr(action, 'tool'):
                            step_text += f"ğŸ› ï¸ **ä½¿ç”¨å·¥å…·:** {action.tool}\n\n"
                        
                        # è®°å½•å·¥å…·è¾“å…¥
                        if hasattr(action, 'tool_input'):
                            step_text += f"ğŸ“ **è¾“å…¥å‚æ•°:** {action.tool_input}\n\n"
                        
                        self.steps.append(step_text)
                        # å®æ—¶æ›´æ–°å½“å‰è¾“å‡º
                        self.current_output = "".join(self.steps)
                        
                    except Exception as e:
                        error_text = f"âš ï¸ **é”™è¯¯:** è¡ŒåŠ¨è®°å½•å‡ºç°é—®é¢˜: {str(e)}\n\n"
                        self.steps.append(error_text)
                        self.current_output = "".join(self.steps)
                
                def on_agent_observation(self, observation, color=None, **kwargs):
                    try:
                        if observation:
                            # ä½¿ç”¨æ›´é†’ç›®çš„æ ¼å¼æ˜¾ç¤ºè§‚å¯Ÿç»“æœ
                            observation_text = f"ğŸ“Š **è§‚å¯Ÿç»“æœ:**\n```\n{observation}\n```\n\n"
                            self.steps.append(observation_text)
                            self.current_output = "".join(self.steps)
                            
                    except Exception as e:
                        error_text = f"âš ï¸ **é”™è¯¯:** è§‚å¯Ÿè®°å½•å‡ºç°é—®é¢˜: {str(e)}\n\n"
                        self.steps.append(error_text)
                        self.current_output = "".join(self.steps)
                
                def on_agent_finish(self, finish, color=None, **kwargs):
                    try:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ç»ˆç­”æ¡ˆ
                        if hasattr(finish, 'return_values'):
                            if isinstance(finish.return_values, dict) and "output" in finish.return_values:
                                final_answer = finish.return_values['output']
                            else:
                                final_answer = str(finish.return_values)
                            
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«"æ€è€ƒ:"å‰ç¼€
                            if "æ€è€ƒ:" in final_answer:
                                # æå–å®é™…ç­”æ¡ˆï¼ˆå»é™¤æ€è€ƒéƒ¨åˆ†ï¼‰
                                answer_parts = final_answer.split("æ€è€ƒ:")
                                if len(answer_parts) > 1:
                                    # å–æœ€åä¸€ä¸ªåˆ†å‰²éƒ¨åˆ†ä½œä¸ºå®é™…ç­”æ¡ˆ
                                    actual_answer = answer_parts[0].strip()
                                else:
                                    actual_answer = final_answer
                            else:
                                actual_answer = final_answer
                            
                            # æ ¼å¼åŒ–æœ€ç»ˆè¾“å‡º
                            formatted_output = f"### âœ¨ æœ€ç»ˆç­”æ¡ˆ:\n\n{actual_answer}\n\n"
                            self.steps.append(formatted_output)
                            
                        self.current_output = "".join(self.steps)
                        
                    except Exception as e:
                        error_text = f"âš ï¸ **é”™è¯¯:** å®Œæˆè®°å½•å‡ºç°é—®é¢˜: {str(e)}\n\n"
                        self.steps.append(error_text)
                        self.current_output = "".join(self.steps)

                def get_current_output(self):
                    """è¿”å›å½“å‰çš„è¾“å‡ºå†…å®¹"""
                    return self.current_output

            handler = VerboseHandler()
            
            # ä¿®æ”¹ agent_chain é…ç½®
            agent_chain = AgentExecutor.from_agent_and_tools(
                agent=agent,
                tools=self.tools,
                verbose=True,
                memory=self.memory,
                max_iterations=3,
                handle_parsing_errors=True,
                early_stopping_method="generate",
                callbacks=[handler],
                return_intermediate_steps=True
            )

            try:
                # è·å–èŠå¤©å†å²
                chat_history = self.memory.load_memory_variables({})["chat_history"]
                
                # åˆ›å»ºå¤„ç†å™¨å®ä¾‹
                handler = VerboseHandler()
                
                # è¿è¡Œagent_chain
                result = agent_chain(
                    {"input": message, "chat_history": chat_history},
                    include_run_info=True
                )
                
                # ä½¿ç”¨å¤„ç†å™¨è·å–å®Œæ•´è¾“å‡º
                full_output = handler.get_current_output()
                
                if not full_output:  # å¦‚æœæ²¡æœ‰æ•è·åˆ°æ­¥éª¤ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
                    steps = []
                    if "intermediate_steps" in result:
                        for step in result["intermediate_steps"]:
                            if len(step) >= 2:
                                action, observation = step
                                steps.append(f"**æ€è€ƒ:** {action.log if hasattr(action, 'log') else ''}")
                                steps.append(f"**è¡ŒåŠ¨:** {action.tool if hasattr(action, 'tool') else ''}")
                                steps.append(f"**è¾“å…¥:** {action.tool_input if hasattr(action, 'tool_input') else ''}")
                                steps.append(f"**è§‚å¯Ÿç»“æœ:**\n{observation}")
                    
                    if "output" in result:
                        steps.append(f"**æœ€ç»ˆç­”æ¡ˆ:**\n{result['output']}")
                    
                    full_output = "\n".join(steps)
                
                yield full_output

            except Exception as e:
                error_msg = f"Agentæ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
                logger.error(error_msg)
                yield error_msg
        elif llm_Agent_checkbox_group == "Simple Chat":
            try:
                # ä½¿ç”¨æ¨¡å‹é…ç½®ç®¡ç†å™¨è·å–LLM
                self.llm = self.get_llm()
                
                # åˆ›å»ºä¸€ä¸ªç®€å•çš„æç¤ºæ¨¡æ¿ï¼Œç¡®ä¿æ­£ç¡®å¤„ç†ä¸Šä¸‹æ–‡
                simple_prompt = ChatPromptTemplate.from_messages([
                    ("system", """ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚è¯·ï¼š
1. ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜
2. ä¿æŒå›ç­”ç®€æ´æ˜äº†
3. å¦‚æœä¸ç¡®å®šï¼Œè¯šå®æ‰¿è®¤
4. ä½¿ç”¨ç¤¼è²Œå‹å¥½çš„è¯­æ°”
5. åªå›ç­”å½“å‰é—®é¢˜ï¼Œä¸è¦é‡å¤ä¹‹å‰çš„å›ç­”"""),
                    MessagesPlaceholder(variable_name="chat_history"),
                    ("user", "{input}")  # å°†ç”¨æˆ·è¾“å…¥ç§»åˆ°æœ€åï¼Œç¡®ä¿æ˜¯æœ€æ–°çš„é—®é¢˜
                ])
                
                # åˆ›å»ºç®€å•çš„å¯¹è¯é“¾
                chat_chain = simple_prompt | self.llm
                
                # è·å–èŠå¤©å†å²å¹¶æ ¼å¼åŒ–
                chat_history = self.memory.load_memory_variables({})["chat_history"]
                
                # æ‰§è¡Œå¯¹è¯
                response = ""
                for chunk in chat_chain.stream({
                    "input": message,
                    "chat_history": chat_history[-4:] if chat_history else []  # åªä¿ç•™æœ€è¿‘çš„2è½®å¯¹è¯
                }):
                    response += chunk.content
                    yield response
                
                # æ›´æ–°è®°å¿†å‰æ¸…é™¤æ—§çš„ä¸Šä¸‹æ–‡
                if len(chat_history) > 4:  # å¦‚æœå†å²è®°å½•è¶…è¿‡2è½®å¯¹è¯
                    self.memory.clear()  # æ¸…é™¤æ‰€æœ‰å†å²
                    # åªä¿å­˜æœ€è¿‘çš„ä¸€è½®å¯¹è¯
                    self.memory.save_context({"input": message}, {"output": response})
                else:
                    # æ­£å¸¸ä¿å­˜ä¸Šä¸‹æ–‡
                    self.memory.save_context({"input": message}, {"output": response})
                
            except Exception as e:
                error_msg = f"Simple Chatæ¨¡å¼æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
                logger.error(error_msg)
                yield error_msg

    def update_collection_name(self):
        # è·å–å·²å­˜åœ¨çš„collectionçš„åç§°åˆ—è¡¨
        collections = self.client.list_collections()
        collection_name_choices = [collection.name for collection in collections]
        # è¿”å›æ–°çš„ä¸‹æ‹‰åˆ—è¡¨ç»„ä»¶
        return gr.Dropdown(
            choices=collection_name_choices,
            value=collection_name_choices[0] if collection_name_choices else None
        )

    def create_interface(self):
        with gr.Blocks(theme=gr.themes.Soft()) as self.interface:
            # å®šä¹‰è‡ªå®šä¹‰CSSæ ·å¼
            custom_css = """
                <style>
                    /* æ ‡é¢˜æ ·å¼ */
                    .gradio-header h1 {
                        text-align: center;
                        margin-bottom: 1rem;
                        font-family: "Courier New", monospace;
                        background: linear-gradient(135deg, #9400D3, #4B0082, #0000FF, #008000, #FFFF00, #FF7F00, #FF0000);
                        -webkit-background-clip: text;
                        -webkit-text-fill-color: transparent;
                        background-clip: text;
                        color: transparent;
                    }

                    /* èŠå¤©ç•Œé¢å®¹å™¨æ ·å¼ */
                    .gradio-container {
                        min-height: 95vh !important;
                    }

                    /* èŠå¤©è®°å½•åŒºåŸŸæ ·å¼ */
                    .chat-history {
                        height: calc(95vh - 200px) !important;
                        overflow-y: auto;
                    }

                    /* å¸®åŠ©é¢æ¿æ ·å¼ */
                    .help-panel {
                        padding: 15px;
                        background: #f8f9fa;
                        border-radius: 8px;
                        height: calc(95vh - 40px);
                        overflow-y: auto;
                    }

                    /* é“¾æ¥æ ·å¼ */
                    a {
                        color: #007bff;
                        text-decoration: none;
                    }

                    a:hover {
                        text-decoration: underline;
                    }

                    /* åˆ†å‰²çº¿æ ·å¼ */
                    hr {
                        border: 0;
                        height: 1px;
                        background: #dee2e6;
                        margin: 1rem 0;
                    }

                    /* å·¥å…·é€‰æ‹©ç»„æ ·å¼ */
                    .tool-group {
                        margin-bottom: 1rem;
                        padding: 10px;
                        border-radius: 5px;
                        background: #ffffff;
                    }
                </style>
            """

            with gr.Row(equal_height=True):
                with gr.Column(scale=3):
                    # å·¦ä¾§åˆ—: æ‰€æ§ä»¶
                    with gr.Row():
                        with gr.Group():
                            gr.Markdown("### Agent Settings")
                            llm_Agent = ["Simple Chat", "openai-functions", "ZeroShotAgent-memory"]
                            llm_Agent_checkbox_group = gr.Dropdown(
                                llm_Agent, 
                                label="LLM Agent Type Options",
                                value=llm_Agent[0]
                            )

                    with gr.Group():
                        gr.Markdown("### Knowledge Collection Settings")
                        collection_name_select = gr.Dropdown(
                            choices=[],
                            label="Select existed Collection",
                            value=None
                        )
                        Refresh_button = gr.Button("Refresh Collection", variant="secondary")
                        Refresh_button.click(fn=self.update_collection_name, outputs=collection_name_select)

                        print_speed_step = gr.Slider(5, 10, label="Print Speed Step", step=1)

                    with gr.Row():
                        with gr.Group():
                            gr.Markdown("### Additional Tools")
                            tool_options = ["Google Search", "Local Knowledge Search", "wolfram-alpha", "arxiv",
                                          "Create Image"]
                            tool_checkbox_group = gr.CheckboxGroup(tool_options, label="Tools Select")
                            gr.Markdown("Note: Select the tools you want to use.")

                    with gr.Group():
                        gr.Markdown("### Embedding Data Settings")
                        Embedding_Model_select = gr.Radio(["Openai Embedding", "HuggingFace Embedding"],
                                                          label="Embedding Model Select",
                                                          value="HuggingFace Embedding")
                        local_data_embedding_token_max = gr.Slider(1024, 15360, step=2,
                                                                   label="Embeddings Data Max Tokens",
                                                                   value=2048)
                with gr.Column(scale=5):
                    # ä¸­é—´èŠå¤©ç•Œé¢
                    chatbot = gr.ChatInterface(
                        self.echo,
                        additional_inputs=[collection_name_select, print_speed_step,
                                         tool_checkbox_group, Embedding_Model_select,
                                         local_data_embedding_token_max, llm_Agent_checkbox_group],
                        title="RainbowGPT-Agent",
                        css=custom_css,
                        theme="soft",
                        fill_height=True,
                        autoscroll=True,
                        type='messages'
                    )

            with gr.Column(scale=2):
                # å³ä¾§å¸®åŠ©é¢æ¿
                with gr.Group(elem_classes="help-panel"):
                    gr.Markdown("""
                        ### ğŸŒˆ RainbowGPT-Agent ä½¿ç”¨æŒ‡å—
                                
                        #### ğŸ¯ ä½¿ç”¨æŠ€å·§
                        
                        **1. æé—®æŠ€å·§**
                        - é—®é¢˜è¦æ¸…æ™°å…·ä½“
                        - å¤æ‚é—®é¢˜å¯ä»¥åˆ†æ­¥æé—®
                        - å¯ä»¥è¿½é—®ä»¥è·å–æ›´è¯¦ç»†ä¿¡æ¯
                        
                        **2. å·¥å…·ä½¿ç”¨**
                        - å¯ä»¥åŒæ—¶é€‰æ‹©å¤šä¸ªå·¥å…·
                        - ç³»ç»Ÿä¼šè‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„å·¥å…·
                        - ä¸åŒå·¥å…·å¯ä»¥ååŒå·¥ä½œ
                        
                        **3. å¯¹è¯ä¼˜åŒ–**
                        - ä¿æŒå¯¹è¯ä¸Šä¸‹æ–‡è¿è´¯
                        - å¯ä»¥å‚è€ƒä¹‹å‰çš„å¯¹è¯å†å²
                        - éœ€è¦æ—¶å¯ä»¥è¯·æ±‚æ¾„æ¸…æˆ–è¡¥å……
                        
                        **4. æ€§èƒ½ä¼˜åŒ–**
                        - é€‰æ‹©åˆé€‚çš„Embeddingæ¨¡å‹
                        - é€‚å½“è°ƒæ•´Tokené™åˆ¶
                        - æ ¹æ®éœ€æ±‚é€‰æ‹©Agentæ¨¡å¼
                        
                        #### ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ
                        - é‡åˆ°é—®é¢˜è¯·è”ç³»ï¼š[zhujiadongvip@163.com](mailto:zhujiadongvip@163.com)
                        - å»ºè®®ä¼˜å…ˆæŸ¥çœ‹ä½¿ç”¨æŠ€å·§è§£å†³é—®é¢˜
                        - æ¬¢è¿åé¦ˆä½¿ç”¨ä½“éªŒ
                    """)

    def launch(self):
        return self.interface
